{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML in Cybersecurity: Project II\n",
    "\n",
    "## Team\n",
    "  * **Team name**:  MMM\n",
    "  * **Members**:  Maria Sargsyan (<email here>), Muneeb Aadil (2581794, maadil@mpi-inf.mpg.de), Muhammad Yaseen (2577833, myaseen@mpi-inf.mpg.de).\n",
    "\n",
    "\n",
    "## Logistics\n",
    "  * **Due date**: 28th November 2019, 13:59:59 (right before the lecture)\n",
    "  * Email the completed notebook to: `mlcysec_ws1920_staff@lists.cispa.saarland`\n",
    "  * Complete this in **teams of 3**\n",
    "  * Feel free to use the course [mailing list](https://lists.cispa.saarland/listinfo/mlcysec_ws1920_stud) to discuss.\n",
    "  \n",
    "## Timeline\n",
    "  * 14-Nov-2019: Project 2 hand-out\n",
    "  * **28-Nov-2019** (13:59:59): Email completed notebook\n",
    "  * 5-Nov-2019: Project 2 discussion and summary\n",
    "  \n",
    "  \n",
    "## About this Project\n",
    "In this project, you will explore an application of ML to a popular task in cybersecurity: malware classification.\n",
    "You will be presented with precomputed behaviour analysis reports of thousands of program binaries, many of which are malwares.\n",
    "Your goal will be train a malware detector using this behavioural reports.\n",
    "\n",
    "\n",
    "## A Note on Grading\n",
    "The grading for this project will depend on:\n",
    " 1. Vectorizing Inputs\n",
    "   * Obtaining a reasonable vectorized representations of the input data (a file containing a sequence of system calls)\n",
    "   * Understanding the influence these representations have on your model\n",
    " 1. Classification Model  \n",
    "   * Following a clear ML pipeline\n",
    "   * Obtaining reasonable performances (>60\\%) on held-out test set\n",
    "   * Choice of evaluation metric\n",
    "   * Visualizing loss/accuracy curves\n",
    " 1. Analysis\n",
    "   * Which methods (input representations/ML models) work better than the rest and why?\n",
    "   * Which hyper-parameters and design-choices were important in each of your methods?\n",
    "   * Quantifying influence of these hyper-parameters on loss and/or validation accuracies\n",
    "   * Trade-offs between methods, hyper-parameters, design-choices\n",
    "   * Anything else you find interesting (this part is open-ended)\n",
    "\n",
    "\n",
    "## Grading Details\n",
    " * 40 points: Vectorizing input data (each input = behaviour analysis file in our case)\n",
    " * 40 points: Training a classification model\n",
    " * 15 points: Analysis/Discussion\n",
    " * 5 points: Clean code\n",
    " \n",
    "## Filling-in the Notebook\n",
    "You'll be submitting this very notebook that is filled-in with your code and analysis. Make sure you submit one that has been previously executed in-order. (So that results/graphs are already visible upon opening it). \n",
    "\n",
    "The notebook you submit **should compile** (or should be self-contained and sufficiently commented). Check tutorial 1 on how to set up the Python3 environment.\n",
    "\n",
    "\n",
    "**The notebook is your project report. So, to make the report readable, omit code for techniques/models/things that did not work. You can use final summary to provide report about these codes.**\n",
    "\n",
    "It is extremely important that you **do not** re-order the existing sections. Apart from that, the code blocks that you need to fill-in are given by:\n",
    "```\n",
    "#\n",
    "#\n",
    "# ------- Your Code -------\n",
    "#\n",
    "#\n",
    "```\n",
    "Feel free to break this into multiple-cells. It's even better if you interleave explanations and code-blocks so that the entire notebook forms a readable \"story\".\n",
    "\n",
    "\n",
    "## Code of Honor\n",
    "We encourage discussing ideas and concepts with other students to help you learn and better understand the course content. However, the work you submit and present **must be original** and demonstrate your effort in solving the presented problems. **We will not tolerate** blatantly using existing solutions (such as from the internet), improper collaboration (e.g., sharing code or experimental data between groups) and plagiarism. If the honor code is not met, no points will be awarded.\n",
    "\n",
    " \n",
    " ## Versions\n",
    "  * v1.1: Updated deadline\n",
    "  * v1.0: Initial notebook\n",
    "  \n",
    "  ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import json \n",
    "import time \n",
    "import pickle \n",
    "import sys \n",
    "import csv \n",
    "import os \n",
    "import os.path as osp \n",
    "import shutil \n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import display, HTML\n",
    " \n",
    "#%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots \n",
    "plt.rcParams['image.interpolation'] = 'nearest' \n",
    "plt.rcParams['image.cmap'] = 'gray' \n",
    " \n",
    "# for auto-reloading external modules \n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython \n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some suggestions of our libraries that might be helpful for this project\n",
    "from collections import Counter          # an even easier way to count\n",
    "from multiprocessing import Pool         # for multiprocessing\n",
    "from tqdm import tqdm                    # fancy progress bars\n",
    "  \n",
    "# Load other libraries here.\n",
    "# Keep it minimal! We should be easily able to reproduce your code.\n",
    "# We only support sklearn and pytorch.\n",
    "\n",
    "# We preload pytorch as an example\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mode = 'cpu'\n",
    "\n",
    "if compute_mode == 'cpu':\n",
    "    device = torch.device('cpu')\n",
    "elif compute_mode == 'gpu':\n",
    "    # If you are using pytorch on the GPU cluster, you have to manually specify which GPU device to use\n",
    "    # It is extremely important that you *do not* spawn multi-GPU jobs.\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'    # Set device ID here\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    raise ValueError('Unrecognized compute mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "  * Download the datasets: [train](https://nextcloud.mpi-klsb.mpg.de/index.php/s/pJrRGzm2So2PMZm) (128M) and [test](https://nextcloud.mpi-klsb.mpg.de/index.php/s/zN3yeWzQB3i5WqE) (92M)\n",
    "  * Unpack them under `./data/train` and `./data/test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that you are prepared with the data\n",
    "#! printf '# train examples (Should be 13682) : '; ls data/train | wc -l\n",
    "#! printf '# test  examples (Should be 10000) : '; ls data/test | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you're set, let's briefly look at the data you have been handed.\n",
    "Each file encodes the behavior report of a program (potentially a malware), using an encoding scheme called \"The Malware Instruction Set\" (MIST for short).\n",
    "At this point, we highly recommend you briefly read-up Sec. 2 of the [MIST](http://www.mlsec.org/malheur/docs/mist-tr.pdf) documentation.\n",
    "\n",
    "You will find each file named as `filename.<malwarename>`:\n",
    "```\n",
    "» ls data/train | head\n",
    "00005ecc06ae3e489042e979717bb1455f17ac9d.NothingFound\n",
    "0008e3d188483aeae0de62d8d3a1479bd63ed8c9.Basun\n",
    "000d2eea77ee037b7ef99586eb2f1433991baca9.Patched\n",
    "000d996fa8f3c83c1c5568687bb3883a543ec874.Basun\n",
    "0010f78d3ffee61101068a0722e09a98959a5f2c.Basun\n",
    "0013cd0a8febd88bfc4333e20486bd1a9816fcbf.Basun\n",
    "0014aca72eb88a7f20fce5a4e000c1f7fff4958a.Texel\n",
    "001ffc75f24a0ae63a7033a01b8152ba371f6154.Texel\n",
    "0022d6ba67d556b931e3ab26abcd7490393703c4.Basun\n",
    "0028c307a125cf0fdc97d7a1ffce118c6e560a70.Swizzor\n",
    "...\n",
    "```\n",
    "and within each file, you will see a sequence of individual systems calls monitored duing the run-time of the binary - a malware named 'Basun' in the case:\n",
    "```\n",
    "» head data/train/000d996fa8f3c83c1c5568687bb3883a543ec874.Basun\n",
    "# process 000006c8 0000066a 022c82f4 00000000 thread 0001 #\n",
    "02 01 | 000006c8 0000066a 00015000\n",
    "02 02 | 00006b2c 047c8042 000b9000\n",
    "02 02 | 00006b2c 047c8042 00108000\n",
    "02 02 | 00006b2c 047c8042 00153000\n",
    "02 02 | 00006b2c 047c8042 00091000\n",
    "02 02 | 00006b2c 047c8042 00049000\n",
    "02 02 | 00006b2c 047c8042 000aa000\n",
    "02 02 | 00006b2c 047c8042 00092000\n",
    "02 02 | 00006b2c 047c8042 00011000\n",
    "...\n",
    "```\n",
    "(**Note**: Please ignore the first line that begins with `# process ...`.)\n",
    "\n",
    "Your task in this project is to train a malware detector, which given the sequence of system calls (in the MIST-formatted file like above), predicts one of 10 classes: `{ Agent, Allaple, AutoIt, Basun, NothingFound, Patched, Swizzor, Texel, VB, Virut }`, where `NothingFound` roughly represents no malware is present.\n",
    "In terms of machine learning terminology, your malware detector $F: X \\rightarrow Y$ should learn a mapping from the MIST-encoded behaviour report (the input $x \\in X$) to the malware class $y \\in Y$.\n",
    "\n",
    "Consequently, you will primarily tackle two challenges in this project:\n",
    "  1. \"Vectorizing\" the input data i.e., representing each input (file) as a tensor\n",
    "  1. Training an ML model\n",
    "  \n",
    "\n",
    "### Some tips:\n",
    "  * Begin with an extremely simple representation/ML model and get above chance-level classification performance\n",
    "  * Choose your evaluation metric wisely\n",
    "  * Save intermediate computations (e.g., a token to index mapping). This will avoid you parsing the entire dataset for every experiment\n",
    "  * Try using `multiprocessing.Pool` to parallelize your `for` loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vectorize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "# We wrote our own PyTorch data loader\n",
    "\n",
    "\n",
    "CLASSES = [\"NothingFound\", \"Basun\", \"Agent\", \"Allaple\", \"AutoIt\",\n",
    "           \"Patched\", \"Swizzor\", \"Texel\", \"VB\", \"Virut\"]\n",
    "\n",
    "class MalwareDataset(Dataset):\n",
    "    def __init__(self, root_dir, dataset_type=\"train\", transform=None):\n",
    "\n",
    "        assert dataset_type in [\"train\",\"test\",\"val\"]\n",
    "        db_path = os.path.join(root_dir, dataset_type + \".csv\")\n",
    "        self.malware_db = pd.read_csv(db_path)\n",
    "        self.dataset_type = dataset_type if dataset_type != 'val' else 'train'\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # TODO: put CLASSES variable inside self.\n",
    "        self.num_classes = len(CLASSES)\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.malware_db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "\n",
    "        # get the file name for this index\n",
    "        mw_filename = self.malware_db.iloc[index,0] + \".\" + self.malware_db.iloc[index,1]\n",
    "\n",
    "        # load and clean the data\n",
    "\n",
    "        # set the corresponding location to 1\n",
    "        label = [0]*len(CLASSES)\n",
    "        readable_label = self.malware_db.iloc[index,1]\n",
    "        label[self.get_class_index(readable_label)] = 1\n",
    "        label_idx = self.get_class_index(readable_label)\n",
    "\n",
    "        fq_path = os.path.join(self.root_dir, self.dataset_type, mw_filename)\n",
    "        contents = open(os.path.join(self.root_dir, self.dataset_type, mw_filename)).readlines()\n",
    "        contents_sanitized = [l.replace(' |',' ').strip(' \\n')  for l in contents if not '#' in l]\n",
    "        sample = {'trace': contents_sanitized, 'label': label, 'readable_label': readable_label,\n",
    "                  'label_idx': label_idx, \n",
    "                  'fq_path': fq_path        # reqd for vectorizer\n",
    "                  }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def get_class_index(self, label):\n",
    "\n",
    "        \"\"\"\n",
    "        Just a simple way to vectorize the output labels using one-hot\n",
    "        encoding at the load time\n",
    "        \"\"\"\n",
    "        assert label in CLASSES\n",
    "        return CLASSES.index(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b. Vectorize: Setup\n",
    "\n",
    "Make one pass over the inputs to identify relevant features/tokens.\n",
    "\n",
    "Suggestion:\n",
    "  - identify tokens (e.g., unigrams, bigrams)\n",
    "  - create a token -> index (int) mapping. Note that you might have a >10K unique tokens. So, you will have to choose a suitable \"vocabulary\" size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c. Vectorize Data\n",
    "\n",
    "Use the (token $\\rightarrow$ index) mapping you created before to vectorize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Transforms\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Some helpful data transformations for the MIST-format\n",
    "\"\"\"\n",
    "\n",
    "class CategoryOperationTransform(object):\n",
    "    \"\"\"\n",
    "    This transform just returns the category and operation part of the trace as feature.\n",
    "    It is useful for making the baseline model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_feats = None\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        # get all the cat/ops identifiers, they are in position 0 and 1\n",
    "        # in each command\n",
    "\n",
    "        cat_ops_list = []\n",
    "\n",
    "        for cmd in sample['trace']:\n",
    "\n",
    "            cmd_parts = cmd.split(' ')\n",
    "            # convert from hex to int\n",
    "            cat, op = int(cmd_parts[0], 16), int(cmd_parts[1], 16)\n",
    "\n",
    "            cat_ops_list.append( (cat, op) )\n",
    "\n",
    "        return { 'trace': cat_ops_list, 'label': sample['label'],'readable_label': sample['readable_label'] }\n",
    "\n",
    "\n",
    "class CategoryCount(object):\n",
    "    \"\"\"\n",
    "    This also only takes into account the category and operation part of the trace as a feature.\n",
    "    Then, it uses one-hot encoding to represent how many rows got the same feature. Lastly, it sums\n",
    "    along the 'line' dimension to get a vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, debug=False):\n",
    "        self.num_categories = 20\n",
    "        self.max_sys_call = 15\n",
    "        self.num_feats = self.num_categories + self.max_sys_call # 20 category of system calls, and\n",
    "        # 15 is the maximum number of system calls\n",
    "        # self.encoder = OneHotEncoder('')\n",
    "        self.debug = debug\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sample (dict): a sample dictionary containing 'trace', 'readable_label', and 'label' as keys.\n",
    "\n",
    "        Returns:\n",
    "            transformed_sample (dict)\n",
    "        \"\"\"\n",
    "\n",
    "        trace = sample['trace']\n",
    "\n",
    "        out = np.zeros(shape=(len(trace), self.num_feats))\n",
    "        for i, cmd in enumerate(trace):\n",
    "            cmd_parsed = cmd.split(' ')\n",
    "            category = int(cmd_parsed[0], 16)\n",
    "            sys_call = int(cmd_parsed[1], 16)\n",
    "\n",
    "            try:\n",
    "                out[i, category] = 1.\n",
    "                out[i, 20 + sys_call] = 1.\n",
    "            except:\n",
    "                print(\"category: {}, 20+sys_call: {}, category hex\".format(category, 20 + sys_call))\n",
    "\n",
    "        # only replacing the old list of strings with transformed vector.\n",
    "        sample['trace'] = np.sum(out, axis=0).astype(np.float32) # sum over the commands to get a vector.\n",
    "        return sample\n",
    "\n",
    "\n",
    "class FirstKArgs(object):\n",
    "    \"\"\"\n",
    "    This transformer takes into account the first two columns (as previously), plus the first K\n",
    "    argument colummns after the first two. The rationale is that, according to MIST documentation,\n",
    "    earlier arguments are more important and significant than later ones (since later ones provide\n",
    "    very thin details.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, K=5):\n",
    "        self.K = K\n",
    "        self.num_categories = 20\n",
    "        self.max_sys_call = 14\n",
    "        self.num_feats = None # decide this.\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        pass\n",
    "\n",
    "    def prepare_tokens(self, data):\n",
    "        pass\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "class HistogramTransform(object):\n",
    "\n",
    "    \"\"\"\n",
    "    `vectorizer_file_path should be a scikit vectorizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vectorizer_file_path):\n",
    "        \n",
    "        self.vectorizer_file_path = vectorizer_file_path\n",
    "        self.vectorizer = pickle.load(open(vectorizer_file_path,'rb'))\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        sample['trace'] = self.vectorizer.transform([open(sample['fq_path'])])\n",
    "        return sample\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train Model\n",
    "\n",
    "You will now train an ML model on the vectorized datasets you created previously.\n",
    "\n",
    "_Note_: Although I often refer to each input as a 'vector' for simplicity, each of your inputs can also be higher dimensional tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a. Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: THIS CLASSES LIST IS COPIED FROM MLW_LOADER.\n",
    "CLASSES = [\"NothingFound\", \"Basun\", \"Agent\", \"Allaple\", \"AutoIt\", \n",
    "           \"Patched\", \"Swizzor\", \"Texel\", \"VB\", \"Virut\"]\n",
    "\n",
    "def evaluate_preds(y_gt, y_pred):\n",
    "    pass\n",
    "\n",
    "def save_model(model, out_path):\n",
    "    pass\n",
    "\n",
    "\n",
    "def find_class_occurences(dir_path='./data/train'):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with having class name as key, and the number of instances\n",
    "    having the class as its corresponding key.\n",
    "    \n",
    "    Args:\n",
    "        dir_path (string): folder containing programs' stack trace.\n",
    "    \n",
    "    Returns:\n",
    "        out (dict): (k,v) pairs where k = class name, v = number of occurences of that class.\n",
    "    \"\"\"\n",
    "    out = dict()\n",
    "    for program_file in tqdm(os.listdir(dir_path)):\n",
    "        class_name = program_file.split('.')[-1]\n",
    "        out[class_name] = out.get(class_name, 0) + 1\n",
    "    return out\n",
    "        \n",
    "def get_class_weights(occ):\n",
    "    \"\"\"\n",
    "    Given class occurences dictionary, it returns a numpy array containing the respective\n",
    "    weight for each class.\n",
    "    \n",
    "    Args:\n",
    "        occ (dict): (k,v) pairs where k = class name, v = number of occurences of that class.\n",
    "        \n",
    "    Returns:\n",
    "        out (np.array) = an array of shape (num_classes,) containing the weight of each class.\n",
    "    \"\"\"\n",
    "    # finding total number of examples in the set.\n",
    "    total = 0.0\n",
    "    for _, v in occ.items():\n",
    "        total += v\n",
    "    \n",
    "    out = np.zeros((len(CLASSES), ))\n",
    "    for k, v in occ.items():\n",
    "        weight = 1. / (occ[k] / total)\n",
    "        out[CLASSES.index(k)] = weight\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:00<00:00, 29862.82it/s]\n"
     ]
    }
   ],
   "source": [
    "class_occ = find_class_occurences(dir_path=\"./data/train\")\n",
    "class_weights = get_class_weights(class_occ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a.1 Define Dataset and Transformation to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset.\n",
    "import malware_loader as mlw_loader\n",
    "import malware_transforms as mlw_transforms\n",
    "\n",
    "vectorizer = mlw_transforms.CategoryCount()\n",
    "\n",
    "\n",
    "data_path = \"./data\"\n",
    "#data_path = \"./data-debug\"\n",
    "\n",
    "train_dataset = mlw_loader.MalwareDataset(root_dir=data_path, dataset_type='train', transform=vectorizer)\n",
    "test_dataset = mlw_loader.MalwareDataset(root_dir=data_path, dataset_type='test', transform=vectorizer)\n",
    "val_dataset = mlw_loader.MalwareDataset(root_dir=data_path, dataset_type='val', transform=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some dataset dependent features.\n",
    "num_classes = train_dataset.num_classes\n",
    "num_feats = vectorizer.num_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please see models.py file for models definition\n",
    "\n",
    "import anns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.c. Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your hyperparameters here\n",
    "\n",
    "# model instantiation\n",
    "\n",
    "expt_name = 'baseline.model'\n",
    "\n",
    "hidden_layers = [128, 256, 128, 64, 32, 16]\n",
    "\n",
    "# Optimization\n",
    "n_epochs = 1\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "momentum = 0.9\n",
    "num_workers = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.d. Train your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "model = anns.ANN(input_dim=num_feats, hidden_layers=hidden_layers,\n",
    "                 output_dim=num_classes).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loader, model, criterion, optimizer, device, print_every=10):\n",
    "    \"\"\"Trains the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    loader_iterable = tqdm(loader)\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, sample in enumerate(loader_iterable):\n",
    "        X_batch, Y_batch = sample['trace'], sample['label_idx']\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "        Y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        running_loss = running_loss + loss.data\n",
    "        \n",
    "        if (i % print_every) == 0:\n",
    "            pass\n",
    "        #print(\"Loss at iteration {}: {}\".format(i, loss.data))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = running_loss / len(loader_iterable)\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(loader, model, criterion, device, print_every=10):\n",
    "    \"\"\"Validates the trained model\"\"\"\n",
    "    model.eval()\n",
    "    loader_iterable = tqdm(loader)\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, sample in enumerate(loader_iterable):\n",
    "        X_batch, Y_batch = sample['trace'], sample['label_idx']\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "        Y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(Y_pred, Y_batch)\n",
    "        running_loss += loss.data\n",
    "\n",
    "        if (i % print_every) == 0:\n",
    "            pass\n",
    "        #print(\"Loss at iteration {}: {}\".format(i, loss.data))\n",
    "\n",
    "    mean_loss = running_loss / len(loader_iterable)\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 1]\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ffa846137a0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 926, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 906, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/myaseen/anaconda3/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/myaseen/project2/malware_loader.py\", line 31, in __getitem__\n    mw_filename = self.malware_db.iloc[index,0] + \".\" + self.malware_db.iloc[index,1]\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1418, in __getitem__\n    return self._getitem_tuple(key)\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2092, in _getitem_tuple\n    self._has_valid_tuple(tup)\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 235, in _has_valid_tuple\n    self._validate_key(k, i)\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2014, in _validate_key\n    self._validate_integer(key, axis)\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2088, in _validate_integer\n    raise IndexError(\"single positional indexer is out-of-bounds\")\nIndexError: single positional indexer is out-of-bounds\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-408a7ddb64e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmean_loss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_loss_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_loss_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-5d2566fe4b36>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(loader, model, criterion, optimizer, device, print_every)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trace'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/myaseen/project2/malware_loader.py\", line 31, in __getitem__\n    mw_filename = self.malware_db.iloc[index,0] + \".\" + self.malware_db.iloc[index,1]\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 1418, in __getitem__\n    return self._getitem_tuple(key)\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2092, in _getitem_tuple\n    self._has_valid_tuple(tup)\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 235, in _has_valid_tuple\n    self._validate_key(k, i)\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2014, in _validate_key\n    self._validate_integer(key, axis)\n  File \"/home/myaseen/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py\", line 2088, in _validate_integer\n    raise IndexError(\"single positional indexer is out-of-bounds\")\nIndexError: single positional indexer is out-of-bounds\n"
     ]
    }
   ],
   "source": [
    "train_losses, test_losses = [], []\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch [{} / {}]\".format(epoch+1, n_epochs))\n",
    "\n",
    "    print(\"Training...\")\n",
    "    mean_loss_train = train_epoch(train_loader, model, criterion, optimizer, device)\n",
    "    print(\"Training Loss: {}\".format(mean_loss_train))\n",
    "    train_losses.append(mean_loss_train)\n",
    "\n",
    "    print(\"Validating...\")\n",
    "    mean_loss_test = validate_epoch(val_loader, model, criterion, device)\n",
    "    print(\"Testing Loss: {}\".format(mean_loss_test))\n",
    "    test_losses.append(mean_loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.e. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3053481b94347b7a77bd4b537da7b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=313), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(loader, model):\n",
    "    model.eval()\n",
    "    y_preds, y_gts = [], []\n",
    "\n",
    "    for sample in tqdm(loader):\n",
    "        X_batch, Y_batch = sample['trace'], sample['label_idx']\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "        Y_scores = model(X_batch)\n",
    "        \n",
    "        _, Y_pred = Y_scores.max(1)\n",
    "        \n",
    "        y_preds.append(Y_pred)\n",
    "        y_gts.append(Y_batch)\n",
    "        \n",
    "    return torch.cat(y_preds), torch.cat(y_gts)\n",
    "\n",
    "pred_class, actual_class = predict(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3435772851503188"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation Metric: F1 score.\n",
    "f1score = f1_score(actual_class, pred_class.numpy(), average=None)\n",
    "\n",
    "mean_f1score = f1score.mean()\n",
    "print(\"mean f1 Score: {}\".format(mean_f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.f. Save Model + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to edit anything in this block\n",
    "\n",
    "save_path = '{0}-checkpoint'.format(expt_name)\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "with open(\"history.pkl\",\"wb\") as fn:\n",
    "    pickle.dump({'train_losses': train_losses,\n",
    "                'test_losses': test_losses,\n",
    "                'mean_f1score': mean_f1score}, fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a. Summary: Main Results\n",
    "\n",
    "If you tried other approaches, summarize their results here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        | Input Representation | Model | Optimizer | Validation Metric | Test Metric |\n",
    "|--------|----------------------|-------|-----------|-------------------|-------------|\n",
    "| Model1 | Unigram tokens       | MLP   | SGD       | 12.34 %           | 23.45%      |\n",
    "| Model2 (this notebook) |                      |       |           |                   |             |\n",
    "| ...    |                      |       |           |                   |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.b. Discussion\n",
    "\n",
    "Enter your final summary here.\n",
    "\n",
    "For instance, you can address:\n",
    "- What was the performance you obtained with the simplest approach?\n",
    "- Which vectorized input representations helped more than the others?\n",
    "- Which malwares are difficult to detect and why?\n",
    "- Which approach do you recommend to perform malware classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
